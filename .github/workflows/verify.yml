# This workflow verifies new associations.

name: Verify New Associations

on:
  pull_request:
    types: [synchronize]

jobs:
  verify_collection:
    outputs:
      any_test_failed: ${{ steps.test-result.outputs.any_test_failed }}
      any_test_skipped: ${{ steps.test-result.outputs.any_test_skipped }}
      any_test_error: ${{ steps.test-result.outputs.any_test_error }}
      all_tests_passed: ${{ steps.test-result.outputs.all_tests_passed }}
    permissions:
      checks: write
      pull-requests: write
    if: |
      ${{ startsWith(github.head_ref, 'diff/uat') }} ||
      ${{ startsWith(github.head_ref, 'diff/ops') }} ||
      contains(github.event.issue.labels.*.name, 'autotest')
    name: Verify ${{ github.event.pull_request.title }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install Poetry
        uses: abatilo/actions-poetry@v2
        with:
          poetry-version: 2.1.3

      - name: Get Environment and Concept ID
        id: get-env-ccid
        run: |
          echo "test_env=$(python -c "print('${{ github.head_ref }}'.split('/')[1])")" >> $GITHUB_OUTPUT
          echo "concept_id=$(python -c "print('${{ github.head_ref }}'.split('/')[2])")" >> $GITHUB_OUTPUT

      - name: Install Dependencies
        id: install
        run: |
          poetry install

      - name: Execute Tests
        id: run-tests
        working-directory: tests
        env:
          TEST_ENV: ${{ steps.get-env-ccid.outputs.test_env }}
          CONCEPT_ID: ${{ steps.get-env-ccid.outputs.concept_id }}
          CMR_USER: ${{ secrets.CMR_USER }}
          CMR_PASS: ${{ secrets.CMR_PASS }}
        run: |
          poetry run pytest verify_collection.py --concept_id $CONCEPT_ID --env $TEST_ENV --junitxml=$GITHUB_WORKSPACE/test-results/test_report.xml --html=$GITHUB_WORKSPACE/test-results/test_report.html || true

      - name: Convert XML to JSON
        id: run-xml-to-json
        working-directory: tests
        run: |
          poetry run python xml_to_json.py $GITHUB_WORKSPACE/test-results/test_report.xml  $GITHUB_WORKSPACE/test-results/parsed_test_results.json

      - name: Publish Test Results
        id: publish-test
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          report_suite_logs: info
          check_name: Tested with Harmony
          json_file: test-results/test_results.json
          comment_title: Test Results for ${{ steps.get-env-ccid.outputs.concept_id }}
          files: test-results/test_report.xml

      - name: Save Test Outputs
        id: test-result
        if: always()
        run: |
          echo "any_test_failed=${{ fromJSON( steps.publish-test.outputs.json ).stats.tests_fail > 0 }}" >> $GITHUB_OUTPUT
          echo "any_test_skipped=${{ fromJSON( steps.publish-test.outputs.json ).stats.tests_skip > 0 }}" >> $GITHUB_OUTPUT
          echo "any_test_error=${{ fromJSON( steps.publish-test.outputs.json ).stats.tests_error > 0 }}" >> $GITHUB_OUTPUT
          echo "any_test_passed=${{ fromJSON( steps.publish-test.outputs.json ).stats.tests_succ > 0 }}" >> $GITHUB_OUTPUT
          echo "all_tests_succeeded_or_skipped=${{ fromJSON( steps.publish-test.outputs.json ).stats.tests_succ > 0 && fromJSON( steps.publish-test.outputs.json ).stats.tests_fail == 0 && fromJSON( steps.publish-test.outputs.json ).stats.tests_error == 0 }}" >> $GITHUB_ENV

      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results
          path: test-results/**

      - name: Mark Tests as Successful
        uses: LouisBrunner/checks-action@v1.6.1
        if: env.all_tests_succeeded_or_skipped == 'true'
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          name: All Tests Run and Completed (Passed and/or Skipped)
          conclusion: success
          output: |
            {"summary":"No failed or errored tests; at least one test passed, with some skipped tests."}

  tests_pass:
    needs: verify_collection
    name: Merge Pull Request
    runs-on: ubuntu-latest
    if: fromJSON( needs.verify_collection.outputs.all_tests_succeeded_or_skipped )
      - name: Remove Unverified Label
        uses: actions-ecosystem/action-remove-labels@v1
        with:
          labels: |
            unverified

      - name: Add Verified Label
        uses: actions-ecosystem/action-add-labels@v1
        with:
          labels: |
            verified

  tests_fail:
    needs: verify_collection
    name: Fail Pull Request
    runs-on: ubuntu-latest
    if: |
      fromJSON( needs.verify_collection.outputs.any_test_failed ) ||
      fromJSON( needs.verify_collection.outputs.any_test_error )
    steps:
      - name: Download Test Results
        uses: actions/download-artifact@v4
        with:
          name: test-results
          path: test-results

      - name: Extract Labels
        id: extract-apply-labels
        run: |
          labels=$(jq -r '.apply_labels | join("\n")' test-results/parsed_test_results.json)
          {
            echo 'labels<<EOF'
            echo "$labels"
            echo 'EOF'
          } >> "$GITHUB_OUTPUT"

      - name: Remove Unverified Label
        uses: actions-ecosystem/action-remove-labels@v1
        with:
          labels: |
            unverified

      - name: Add Extracted Labels
        uses: actions-ecosystem/action-add-labels@v1
        with:
          labels: ${{ steps.extract-apply-labels.outputs.labels }}

      - name: Add Failure Comment
        uses: mshick/add-pr-comment@v2
        with:
          message: |
            **Action Needed**
            Tests have failed or encountered an unknown error. Open the status checks to view the logs and review the failure.

  #tests_skipped:
  #  needs: verify_collection
  #  name: Update Pull Request
  #  runs-on: ubuntu-latest
  #  if: fromJSON( needs.verify_collection.outputs.any_test_skipped )
  #  steps:
  #    - name: Remove Unverified Label
  #      uses: actions-ecosystem/action-remove-labels@v1
  #      with:
  #        labels: |
  #          unverified
  #    - name: Add Skipped Label
  #      uses: actions-ecosystem/action-add-labels@v1
  #      with:
  #        labels: |
  #          tests skipped
  #    - name: Add Skipped Comment
  #      uses: mshick/add-pr-comment@v2
  #      with:
  #        message: |
  #          **Action Needed**
  #          Unable to verify collection because test(s) were skipped.
  #          Open the status check to view the logs from the test run and determine why tests were skipped.
  #          If there is a justifiable reason, add a comment here and manually merge this PR to accept this unverified association.
  #          Otherwise, fix the problem that caused the test to be skipped and rerun verification.